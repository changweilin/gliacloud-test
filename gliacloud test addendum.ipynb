{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 41504,
     "status": "ok",
     "timestamp": 1524763684046,
     "user": {
      "displayName": "張維尼",
      "photoUrl": "//lh3.googleusercontent.com/-c5EqT2ZqGcE/AAAAAAAAAAI/AAAAAAAACAg/TmR2MAQg0Nc/s50-c-k-no/photo.jpg",
      "userId": "111480244562734483115"
     },
     "user_tz": -480
    },
    "id": "8YLuSval-BM5",
    "outputId": "65089bce-1f6c-4394-b15d-9ee2e2c5e9ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fuse: mountpoint is not empty\r\n",
      "fuse: if you are sure this is safe, use the 'nonempty' mount option\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p Rawdata\n",
    "!google-drive-ocamlfuse Rawdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 25785,
     "status": "ok",
     "timestamp": 1524763801255,
     "user": {
      "displayName": "張維尼",
      "photoUrl": "//lh3.googleusercontent.com/-c5EqT2ZqGcE/AAAAAAAAAAI/AAAAAAAACAg/TmR2MAQg0Nc/s50-c-k-no/photo.jpg",
      "userId": "111480244562734483115"
     },
     "user_tz": -480
    },
    "id": "8erA31My_84J",
    "outputId": "9824e439-2077-4843-9fc5-b8a5baf00455"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2018-04-26 17:29:37--  https://raw.githubusercontent.com/livingbio/DeepLearningTutorial/master/raw_sentences.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2955731 (2.8M) [text/plain]\n",
      "Saving to: ‘Rawdata/gliacloud text/raw_sentences.txt’\n",
      "\n",
      "raw_sentences.txt   100%[===================>]   2.82M   174KB/s    in 20s     \n",
      "\n",
      "2018-04-26 17:30:00 (143 KB/s) - ‘Rawdata/gliacloud text/raw_sentences.txt’ saved [2955731/2955731]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/livingbio/DeepLearningTutorial/master/raw_sentences.txt -P 'Rawdata/gliacloud text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3560,
     "status": "ok",
     "timestamp": 1524764619663,
     "user": {
      "displayName": "張維尼",
      "photoUrl": "//lh3.googleusercontent.com/-c5EqT2ZqGcE/AAAAAAAAAAI/AAAAAAAACAg/TmR2MAQg0Nc/s50-c-k-no/photo.jpg",
      "userId": "111480244562734483115"
     },
     "user_tz": -480
    },
    "id": "hHLB0FfE5Bxi",
    "outputId": "d9925c3b-6d69-4500-f6e2-479b58d963f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting of bigram with \"we\", \"are\": 283.0\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "\n",
    "# The probabilities of each bigram and trigram.\n",
    "# A function ngram_probs to calculate these counts.\n",
    "def ngram_probs(filename='raw_sentences.txt'):\n",
    "    with open(filename,'r') as file:\n",
    "        content_text = file.read()\n",
    "    \n",
    "    tokenizer = Tokenizer(lower=True, split=\" \")\n",
    "    tokenizer.fit_on_texts([content_text])\n",
    "    vocab = tokenizer.word_index\n",
    "    vocab_size = len(vocab)\n",
    "    \n",
    "    vocab_reverse = {}\n",
    "    vocab_keys = list(vocab.keys())\n",
    "    for vocab_word in vocab_keys:\n",
    "        vocab_index = vocab[vocab_word]\n",
    "        vocab_reverse[vocab_index] = vocab_keys[vocab_index-1]\n",
    "            \n",
    "    content_word_ids = tokenizer.texts_to_sequences([content_text])\n",
    "    content_word_ids = content_word_ids[0]\n",
    "    content_len = len(content_word_ids)\n",
    "    \n",
    "    unigram_probs = np.zeros([vocab_size])\n",
    "    bigram_probs = np.zeros([vocab_size, vocab_size])\n",
    "    trigram_probs = np.zeros([vocab_size, vocab_size, vocab_size])\n",
    "    \n",
    "    for i in range(content_len):\n",
    "        # P(W_i)\n",
    "        unigram_probs[content_word_ids[i]-1] += 1\n",
    "        \n",
    "        # P(W_i|W_i-1)\n",
    "        if i>0:\n",
    "            bigram_probs[content_word_ids[i]-1, content_word_ids[i-1]-1] += 1\n",
    "        \n",
    "        # P(W_i|W_i-2, W_i-1)\n",
    "        if i-1>0:\n",
    "            trigram_probs[content_word_ids[i]-1, content_word_ids[i-1]-1, content_word_ids[i-2]-1] += 1\n",
    "    return unigram_probs, bigram_probs, trigram_probs, vocab, vocab_reverse\n",
    "\n",
    "cnt1, cnt2, cnt3, vocab ,vocab_reverse = ngram_probs('E:/Raw data/test/raw_sentences.txt')\n",
    "vocab_size = len(vocab)\n",
    "print('Counting of bigram with \"we\", \"are\":', cnt2[(vocab['we']-1, vocab['are']-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 696,
     "status": "ok",
     "timestamp": 1524765807846,
     "user": {
      "displayName": "張維尼",
      "photoUrl": "//lh3.googleusercontent.com/-c5EqT2ZqGcE/AAAAAAAAAAI/AAAAAAAACAg/TmR2MAQg0Nc/s50-c-k-no/photo.jpg",
      "userId": "111480244562734483115"
     },
     "user_tz": -480
    },
    "id": "HHvB4rlv5Bxn",
    "outputId": "71596dff-9367-4c9d-cc59-938da871d1a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(we) = 0.019591140053225306\n",
      "P(are|we) = 0.19977453901280295\n",
      "P(family|we,are) = 0.004433696090286175\n"
     ]
    }
   ],
   "source": [
    "# Return the distribute of the number of N-grams counting\n",
    "def ngram_distr(cnts):\n",
    "    distr = {}\n",
    "    cnts = cnts.reshape(-1)\n",
    "    for cnt in cnts:\n",
    "        if distr.__contains__(cnt):\n",
    "            distr[cnt] += 1\n",
    "        else:\n",
    "            distr[cnt] = 1\n",
    "    return distr\n",
    "\n",
    "unigram_distr = ngram_distr(cnt1)\n",
    "bigram_distr = ngram_distr(cnt2)\n",
    "trigram_distr = ngram_distr(cnt3)\n",
    "cnt_sum = cnt1.sum()\n",
    "\n",
    "def vocab_idx(word):\n",
    "    return vocab[word]-1\n",
    "\n",
    "# To calculate the probabilities of a word.\n",
    "def prob1(target, cnt1=cnt1):\n",
    "    return cnt1[target] / cnt_sum\n",
    "\n",
    "p = prob1(vocab_idx('we'))\n",
    "print('P(we) =', p)\n",
    "\n",
    "# To calculate the probabilities of next word of a unigram.\n",
    "def prob2(unigram, target, cnt1=cnt1, cnt2=cnt2):\n",
    "    return cnt2[target, unigram] / cnt1[unigram]\n",
    "\n",
    "p = prob2(vocab_idx('we'), vocab_idx('are'))\n",
    "print('P(are|we) =', p)\n",
    "\n",
    "# To calculate the probabilities of next word of a bigram.\n",
    "def prob3(bigram, target, cnt2=cnt2, cnt3=cnt3):\n",
    "    return cnt3[target, bigram[1], bigram[0]] / (cnt2[bigram[1], bigram[0]] + 1e-16)\n",
    "\n",
    "p = prob3([vocab_idx('we'), vocab_idx('are')], vocab_idx('family'))\n",
    "print('P(family|we,are) =', p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 696,
     "status": "ok",
     "timestamp": 1524765807846,
     "user": {
      "displayName": "張維尼",
      "photoUrl": "//lh3.googleusercontent.com/-c5EqT2ZqGcE/AAAAAAAAAAI/AAAAAAAACAg/TmR2MAQg0Nc/s50-c-k-no/photo.jpg",
      "userId": "111480244562734483115"
     },
     "user_tz": -480
    },
    "id": "HHvB4rlv5Bxn",
    "outputId": "71596dff-9367-4c9d-cc59-938da871d1a8"
   },
   "outputs": [],
   "source": [
    "# Discount factor of the probability of the N-gram.\n",
    "def prob_discount(r, distr, K_limit=5):\n",
    "    dr = 1.0\n",
    "    if r < K_limit:\n",
    "        dr = (r + 1) / r * distr[r + 1] / distr[r]\n",
    "    return dr\n",
    "\n",
    "# Bigram back-off factor of Katz Smoothing\n",
    "def backoff2(unigram):\n",
    "    prob2_sum = 0.0\n",
    "    prob1_sum = 0.0\n",
    "    for word in range(len(vocab)):\n",
    "        r = cnt2[word, unigram]\n",
    "        if r>0:\n",
    "            dr = prob_discount(r, bigram_distr)\n",
    "            prob2_sum += dr * prob2(unigram, word)\n",
    "            prob1_sum += prob1(word)\n",
    "    return (1.0 - prob2_sum) / (1.0 - prob1_sum)\n",
    "\n",
    "# Bigram Katz's back-off probobility\n",
    "def prob2_Katz(unigram, target, cnt2=cnt2):\n",
    "    r = cnt2[target, unigram]\n",
    "    if r>0:\n",
    "        return prob_discount(r, bigram_distr) * prob2(unigram, target)\n",
    "    else:\n",
    "        return backoff2(unigram) * prob1(target)\n",
    "\n",
    "# Trigram back-off factor of Katz Smoothing\n",
    "def backoff3(bigram):\n",
    "    prob3_sum = 0.0\n",
    "    prob2_sum = 0.0\n",
    "    for word in range(len(vocab)):\n",
    "        r = cnt3[word, bigram[1], bigram[0]]\n",
    "        if r>0:\n",
    "            dr = prob_discount(r, trigram_distr)\n",
    "            prob3_sum += dr * prob3(bigram, word)\n",
    "            prob2_sum += prob2_Katz(bigram[1], word)\n",
    "    return (1.0 - prob3_sum) / (1.0 - prob2_sum)\n",
    "\n",
    "# Trigram Katz's back-off probobility\n",
    "def prob3_Katz(bigram, target, cnt3=cnt3):\n",
    "    r = cnt3[target, bigram[1], bigram[0]]\n",
    "    if r>0:\n",
    "        return prob_discount(r, trigram_distr) * prob3(bigram, target)\n",
    "    else:\n",
    "        return backoff3(bigram) * prob2_Katz(bigram[1], target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 676,
     "status": "ok",
     "timestamp": 1524765809374,
     "user": {
      "displayName": "張維尼",
      "photoUrl": "//lh3.googleusercontent.com/-c5EqT2ZqGcE/AAAAAAAAAAI/AAAAAAAACAg/TmR2MAQg0Nc/s50-c-k-no/photo.jpg",
      "userId": "111480244562734483115"
     },
     "user_tz": -480
    },
    "id": "jYMMO1O75Bxr",
    "outputId": "1e2edaf1-7f36-4c22-a7b6-0489dc7edba1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we are going nt to know do this nt is it not 's the not\n"
     ]
    }
   ],
   "source": [
    "def predict_step(pre_words):\n",
    "    prob3_max = 0.0\n",
    "    word_max = ''\n",
    "    for word in range(len(vocab)):\n",
    "        this_prob3 = prob3_Katz(pre_words, word)\n",
    "        if prob3_max < this_prob3 or prob3_max == 0.0:\n",
    "            prob3_max = this_prob3\n",
    "            word_max = word\n",
    "    return word_max\n",
    "\n",
    "# Predict the sentence by finding the max likelihood word.\n",
    "def predict_max(starting, cnt2=cnt2, cnt3=cnt3):\n",
    "    list_of_words = starting\n",
    "    while list_of_words[-1]!='.' and len(list_of_words)<15:\n",
    "        word_max = predict_step([list_of_words[-1], list_of_words[-2]])\n",
    "        list_of_words.append(word_max)\n",
    "    return [vocab_reverse[idx+1] for idx in list_of_words]\n",
    "\n",
    "sent = predict_max([vocab_idx('we'), vocab_idx('are')])\n",
    "assert sent[-1] == '.' or len(sent) <= 15\n",
    "print(' '.join(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we are going nt to work do i nt do want you to i do\n",
      "we are going nt to work do i nt do want you to he do\n",
      "we are going nt to work do i nt do want you to but do\n",
      "we are going nt to work do i nt do want you to but me\n",
      "we are going nt to work do i nt do want you to it be\n",
      "we are going nt to work do i nt do know you what do you\n",
      "we are going nt to know do this you is are it you 's and\n",
      "we are going nt to know do this you is know that us 's against\n",
      "we are going nt to work do i nt do know you what want 's\n",
      "we are going nt to work do i nt do know you what want they\n",
      "we are going nt to work do i nt do know you what do i\n",
      "we are going nt to work do i nt do know you what are we\n",
      "we are going nt to work do i nt do know you what do they\n",
      "we are going nt to work do i nt do know you what want is\n",
      "we are going nt to work do i nt do know you what want do\n",
      "we are going nt to work do i nt do want you to but what\n"
     ]
    }
   ],
   "source": [
    "def beam_search_step(pre_words, beam_size):\n",
    "    prob3_beam = [0.0]*beam_size\n",
    "    words_beam = ['']*beam_size\n",
    "    for word in range(len(vocab)):\n",
    "        this_prob3 = prob3_Katz(pre_words, word)\n",
    "        for idx in range(beam_size):\n",
    "            if prob3_beam[idx] < this_prob3:\n",
    "                prob3_temp = prob3_beam[idx]\n",
    "                prob3_beam[idx] = this_prob3\n",
    "                this_prob3 = prob3_temp\n",
    "                word_temp = words_beam[idx]\n",
    "                words_beam[idx] = word\n",
    "                word = word_temp\n",
    "            elif prob3_beam[idx] == 0.0:\n",
    "                prob3_beam[idx] = this_prob3\n",
    "                words_beam[idx] = word\n",
    "            else:\n",
    "                continue\n",
    "    return words_beam, prob3_beam\n",
    "\n",
    "# Predict the sentence by finding the beam search word.\n",
    "def predict_beam(bigram, beam_size=4, sent_length=15, cnt2=cnt2, cnt3=cnt3):\n",
    "    list_of_sentence = [bigram]\n",
    "    list_of_prob_n = [0.0]\n",
    "    while max([len(sentence) for sentence in list_of_sentence])<sent_length:\n",
    "        total_prob_beam = []\n",
    "        total_words_beam = []\n",
    "        for sentence, prob_n in zip(list_of_sentence, list_of_prob_n):\n",
    "            if sentence[-1]!='.':\n",
    "                words_beam, prob_beam = beam_search_step([sentence[-1], sentence[-2]], beam_size)\n",
    "                total_prob_beam += [prob_n + np.log(prob + 1e-16) for prob in prob_beam]\n",
    "                total_words_beam += [sentence + [word] for word in words_beam]\n",
    "            else:\n",
    "                total_prob_beam += [prob_n]\n",
    "                total_words_beam += [sentence]\n",
    "        \n",
    "        total_beam = zip(total_words_beam, total_prob_beam)\n",
    "        total_beam=[(words, prob) for words, prob in zip(total_words_beam, total_prob_beam)]\n",
    "        total_beam.sort(key = lambda t: t[1], reverse=True)\n",
    "        \n",
    "        #print('Sentence:', max([len(sentence) for sentence in list_of_sentence]))\n",
    "        if len(list_of_sentence)<beam_size:\n",
    "            list_of_sentence = [bigram]*beam_size\n",
    "            list_of_prob_n = [0.0]*beam_size\n",
    "        for idx in range(beam_size):\n",
    "            sentence, prob_n = total_beam[idx]\n",
    "            list_of_sentence[idx] = sentence\n",
    "            list_of_prob_n[idx] = prob_n\n",
    "            #print(prob_n, ' '.join(vocab_reverse[idx+1] for idx in sentence))\n",
    "    return [[vocab_reverse[idx+1] for idx in sentence] for sentence in list_of_sentence]\n",
    "\n",
    "for sent in predict_beam([vocab_idx('we'), vocab_idx('are')], beam_size=16):\n",
    "    assert sent[-1] == '.' or len(sent) <= 15\n",
    "    print(' '.join(sent))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "gliacloud test.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
